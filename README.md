# LLM-VT-AL: Enhanced Counterfactual Data Augmentation

A scalable, LLM-based pipeline for generating high-quality counterfactual examples to improve few-shot text classification performance.

## Quick Start

### 1. Setup
```bash
source venv/bin/activate
```

### 2. Configure LLM Provider

**Option A: Use Gemini**
```yaml
# config.yaml
llm:
  provider: gemini
  gemini:
    api_key: YOUR_GEMINI_API_KEY
    model: gemini-2.5-flash  
```

**Option B: Use Azure OpenAI**
```yaml
# config.yaml
llm:
  provider: openai
  openai:
    api_key: YOUR_AZURE_OPENAI_API_KEY
    azure_endpoint: https://your-endpoint.openai.azure.com/openai
    model: gpt-4o  # Or gpt-4, gpt-3.5-turbo
```
**Option C: Use Ollama (Local)**
```bash
ollama serve
ollama pull qwen2.5:7b
```
```yaml
# config.yaml
llm:
  provider: ollama
  ollama:
    model: qwen2.5:7b
```

### 3. Prepare Dataset
Place your CSV files in `input_data/`:
- `emotions_train.csv` (training data)
- `emotions_test.csv` (test data)

Required columns: `id`, `example`, `Label`

### 4. Run Pipeline
```bash
# Activate virtual environment first
source venv/bin/activate

# Run all scripts sequentially
chmod +x run_all.sh
./run_all.sh

# Or run individually
python 01_data_formatting.py
python 02_counterfactual_over_generation.py
python 03_counterfactual_filtering.py
python 04_counterfactual_evaluation.py
```

## üîÑ Pipeline Overview

```
Input: 500 Training Examples
        ‚Üì
Script 01: Enhanced Pattern Identification
    ‚Ä¢ Processes 50 examples per label (vs 10 original)
    ‚Ä¢ Batch processing for efficiency
    ‚Ä¢ Generates ~300 patterns (6x improvement)
        ‚Üì
Script 02: Counterfactual Generation  
    ‚Ä¢ 1,325 counterfactuals generated
    ‚Ä¢ 99.9% success rate
    ‚Ä¢ 2-3x faster with optimized Gemini
        ‚Üì
Script 03: Three-Stage Quality Filtering
    ‚Ä¢ Filter 1 (Heuristic)
    ‚Ä¢ Filter 2 (Semantic) 
    ‚Ä¢ Filter 3 (Discriminator)
    ‚Ä¢ Final: 289 high-quality counterfactuals
        ‚Üì
Script 04: Enhanced Evaluation
    ‚Ä¢ Tests up to 150+ shot classification
    ‚Ä¢ Complete test dataset evaluation 
    ‚Ä¢ Robust metrics: Precision, Recall, F-Score, Accuracy
        ‚Üì
Output: 67% peak accuracy, 6-7x improved data processing scale
```
## üìÅ Project Structure

```
LLM-VT-AL/
‚îú‚îÄ‚îÄ config.yaml                          # Main configuration
‚îú‚îÄ‚îÄ run_all.sh                          # Pipeline automation
‚îú‚îÄ‚îÄ requirements.txt                     # Dependencies
‚îÇ
‚îú‚îÄ‚îÄ utils/                               # Core utilities
‚îÇ   ‚îú‚îÄ‚îÄ llm_provider.py                  # LLM abstraction layer
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py                      # Utility exports
‚îÇ
‚îú‚îÄ‚îÄ 01_data_formatting.py                # Enhanced pattern identification
‚îú‚îÄ‚îÄ 02_counterfactual_over_generation.py # Counterfactual generation  
‚îú‚îÄ‚îÄ 03_counterfactual_filtering.py       # Three-stage filtering
‚îú‚îÄ‚îÄ 04_counterfactual_evaluation.py      # Enhanced evaluation
‚îÇ
‚îú‚îÄ‚îÄ input_data/                          # Your datasets
‚îÇ   ‚îú‚îÄ‚îÄ emotions_train.csv
‚îÇ   ‚îî‚îÄ‚îÄ emotions_test.csv
‚îÇ
‚îî‚îÄ‚îÄ output_data/                         # Generated results
    ‚îú‚îÄ‚îÄ [42]filtered_emotions_train.csv  # 289 high-quality counterfactuals
    ‚îî‚îÄ‚îÄ archive/gpt/                     # Evaluation results
```

## üîß Script Details

### Script 01: Enhanced Data Formatting
**Purpose**: Pattern identification and candidate phrase generation
**Enhancements**: 
- Processes 50 examples per label (vs 10)
- Batch processing for efficiency
- Generates ~300 patterns (6x improvement)

### Script 02: Counterfactual Over-Generation  
**Purpose**: Generate complete counterfactual sentences
**Results**: 1,325 counterfactuals with 99.9% success rate

### Script 03: Counterfactual Filtering
**Purpose**: Three-stage quality control
**Results**: 289 high-quality counterfactuals (35% pass rate)
- Stage 1: Remove meta-responses  
- Stage 2: Validate semantic consistency
- Stage 3: Verify label transformation

### Script 05: Enhanced Evaluation
**Purpose**: Measure counterfactual effectiveness
**Enhancements**:
- Up to 300-shot classification capability
- Complete test dataset evaluation
- Multiple evaluation metrics

## Troubleshooting

### Common Issues

**Gemini Rate Limits**
```bash
# Check if thinking mode is properly disabled
grep "thinking_budget=0" utils/llm_provider.py
```

**Configuration Issues**
```bash
# Verify config structure
python -c "from utils import load_config; print(load_config())"
```

**File Not Found**
```bash
# Check input data exists
ls -la input_data/
```

**Script Dependencies**
Scripts must run in order: 01 ‚Üí 02 ‚Üí 03 ‚Üí 04

## üé® Customization

### Switch LLM Providers
```yaml
# Use Ollama instead of Gemini
llm:
  provider: ollama
  ollama:
    model: qwen2.5:7b
```

### Adjust Processing Scale
```yaml
processing:
  max_examples_per_label: 30    # Reduce for faster processing
  token_limit: 10000000         # Lower budget
  evaluation_shots: [10, 30, 50]  # Fewer evaluation points
```

### Use Different Dataset
```yaml
dataset:
  train_file: your_data_train.csv
  test_file: your_data_test.csv
  columns:
    id: sample_id
    text: sentence  
    label: category
```
## üìù License

This enhanced LLM-VT-AL pipeline builds upon counterfactual data augmentation research and implements optimizations for scalable, production-ready text classification.

---

## Quick Start

### Prerequisites

```bash
# Python 3.8+
python --version

# Install dependencies
pip install -r requierments.txt
```

### Configuration

1. **Create/Edit `config.ini`:**

```ini
[settings]
openai_api = YOUR_OPENAI_API_KEY_HERE
data_file = emotions.csv
test_file = emotions_test.csv
seed = 42
```

2. **Prepare Input Data:**

Place these files in `input_data/`:
- `emotions.csv` - Training data (500 rows)
- `emotions_test.csv` - Test data (100 rows)

**Required schema for training data:**
```csv
id,example,Label
ss104440,"i feel hopeless because i don't know how to fix this","sadness"
ss224831,"i feel so blessed to have such wonderful friends","love"
```

**Required schema for test data:**
```csv
id,example,Label
test001,"i am so excited about this","joy"
test002,"i feel really sad today","sadness"
```

## Expected Outputs

### Script 01 Output Example

**File:** `output_data/[42]annotated_data_with_pattern_emotions.csv`

```csv
id,example,Label,pattern,hihglight
ss104440,"i feel hopeless...","sadness","i feel [CANDIDATE] because [CONTEXT]","hopeless"
ss224831,"i feel blessed...","love","i feel [CANDIDATE] because [CONTEXT]","blessed"
```

**File:** `output_data/interim_output/[42]candidate_phrases_emotions.csv`

```csv
id,ori_text,ori_label,pattern,highlight,candidate_phrases,target_label
ss104440,"i feel hopeless...","sadness","i feel [CANDIDATE]...","hopeless","hopeful","joy"
ss104440,"i feel hopeless...","sadness","i feel [CANDIDATE]...","hopeless","frustrated","anger"
ss104440,"i feel hopeless...","sadness","i feel [CANDIDATE]...","hopeless","terrified","fear"
```

### Script 02 Output Example

**File:** `output_data/[42]counterfactuals_emotions.csv`

```csv
id,ori_text,ori_label,pattern,highlight,candidate_phrases,target_label,counterfactual
ss104440,"i feel hopeless...","sadness","i feel [CANDIDATE]...","hopeless","hopeful","joy","i feel hopeful because i can't wait to celebrate this"
```

### Script 03 Output Example

**File:** `output_data/[42]filtered_emotions.csv`

```csv
id,ori_text,ori_label,pattern,highlight,candidate_phrases,target_label,counterfactual,heuristic_filtered,matched_pattern,is_ori,is_target
ss104440,"i feel hopeless...","sadness","i feel [CANDIDATE]...","hopeless","hopeful","joy","i feel hopeful...","True","True","False","True"
```

**File:** `output_data/[42]fine_tuneset_emotions.csv` (subset where all filters = True)

### Script 04 Output Example

**File:** `output_data/archive/gpt/[42][GPT]_counter_emotions_prf.csv`

```csv
shots,precision,recall,fscore
10,0.52,0.49,0.50
15,0.61,0.58,0.59
30,0.73,0.70,0.71
50,0.81,0.78,0.79
70,0.85,0.82,0.83
90,0.87,0.84,0.85
120,0.89,0.86,0.87
```

---